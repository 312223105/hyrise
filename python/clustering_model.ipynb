{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import json\n",
    "import clustering\n",
    "from clustering.what_if_model import WhatIfModel\n",
    "from clustering.pqp_input_parser import PQPInputParser\n",
    "from clustering.util import create_model\n",
    "import autoreload\n",
    "import clustering.evaluation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_TPCH = \"~/Dokumente/repos/example_plugin/stats/final/tpch/sf10-2d\"\n",
    "TPCH_ORDERKEY = f\"{BASE_PATH_TPCH}/l_orderkey\"\n",
    "TPCH_SHIPDATE = f\"{BASE_PATH_TPCH}/l_shipdate\"\n",
    "TPCH_PARTKEY = f\"{BASE_PATH_TPCH}/l_partkey\"\n",
    "TPCH = {\n",
    "    'l_shipdate': TPCH_SHIPDATE,\n",
    "    'l_orderkey': TPCH_ORDERKEY,\n",
    "    'l_partkey': TPCH_PARTKEY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = create_model(\"lineitem\", PQPInputParser(\"tpch\", TPCH_ORDERKEY), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLUSTERING_COLUMN = \"l_orderkey\"\n",
    "SIDED = True\n",
    "join_results = clustering.evaluation.evaluate_join_step(m, TPCH[CLUSTERING_COLUMN], [CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100], \"ALL\", SIDED)\n",
    "\n",
    "\n",
    "print(f\"There are {len(join_results)} joins\")\n",
    "join_results['e'] = join_results['TOTAL_ERROR'] ** 2\n",
    "join_results.sort_values(['e'], ascending=False)[[\"QUERY_HASH1\", \"DESCRIPTION1\", \"RUNTIME_BASE_MS\", \"RUNTIME_ESTIMATE_MS\", \"RUNTIME_CLUSTERED_MS\", \"RELATIVE_ERROR\", \"TOTAL_ERROR_MS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_join_results = None\n",
    "#old_join_results = join_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.evaluation.print_aggregated_metrics(join_results, m.query_frequencies)\n",
    "fig = clustering.evaluation.plot_join_errors(join_results, old_join_results, m.query_frequencies)\n",
    "#old_join_results = join_results\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joindbg = pd.read_csv(TPCH_SHIPDATE + \"/joins.csv\", sep='|')\n",
    "joindbg.dropna(inplace=True)\n",
    "joindbg['BUILD_COLUMN'] = joindbg.apply(lambda x: x[f\"{x['BUILD_SIDE']}_COLUMN_NAME\"], axis=1)\n",
    "joindbg['PROBE_COLUMN'] = joindbg.apply(lambda x: x[f\"{x['PROBE_SIDE']}_COLUMN_NAME\"], axis=1)\n",
    "viewcols = ['QUERY_HASH', 'IS_FLIPPED', 'PROBE_SORTED', 'BUILD_SORTED', 'PROBE_COLUMN', 'BUILD_COLUMN', 'DESCRIPTION']\n",
    "joindbg[joindbg['QUERY_HASH'] == '3534234c34669919'][viewcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERING_COLUMN =  \"l_partkey\"\n",
    "METRIC = \"PIC\"\n",
    "simulation_results = clustering.evaluation.evaluate_join_simulation(m, TPCH[CLUSTERING_COLUMN], [CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "#simulation_results[['QUERY_HASH1', 'DESCRIPTION1', f\"{METRIC}1\", f\"{METRIC}2\", f\"{METRIC}\"]].sort_values([f\"{METRIC}\"], ascending=False)\n",
    "simulation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCAN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "scan_results = clustering.evaluation.evaluate_scans(m, TPCH[CLUSTERING_COLUMN], [CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "print(f\"There are {len(scan_results)} scans on {m.table_name}\")\n",
    "\n",
    "scan_results['e'] = scan_results['TOTAL_ERROR'] ** 2\n",
    "scan_results['DESC'] = scan_results.apply(lambda x: \" \".join(x['DESCRIPTION1'].split(\" \")[3:]), axis=1)\n",
    "scan_results.sort_values(['e'], ascending=False)[[\"QUERY_HASH\", \"DESC\", \"COLUMN_NAME\", \"RUNTIME_BASE_MS\", \"RUNTIME_ESTIMATE_MS\", \"RUNTIME_CLUSTERED_MS\", \"TOTAL_ERROR_MS\", \"RELATIVE_ERROR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_scan_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.evaluation.print_aggregated_metrics(scan_results, m.query_frequencies)\n",
    "fig = clustering.evaluation.plot_scan_errors(scan_results, old_scan_results, m.query_frequencies)\n",
    "#old_scan_results = scan_results\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scan_results['DESCRIPTION1'].apply(lambda x: \"Like\" in x).any()\n",
    "adapted_scans = m.adapt_scans_to_clustering(m.table_scans.copy(), ['l_shipdate'], 'l_shipdate', [100])\n",
    "adapted_scans = adapted_scans[(adapted_scans['DESCRIPTION'] == \"l_shipdate BETWEEN UPPER EXCLUSIVE '1994-01-01' AND '1995-01-01'\")]\n",
    "adapted_scans[['QUERY_HASH', 'DESCRIPTION']]\n",
    "m.table_scans['DATA_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLUSTERING_COLUMN =  \"l_partkey\"\n",
    "METRIC = \"IR\"\n",
    "simulation_results = clustering.evaluation.evaluate_scan_simulation(m, TPCH_PARTKEY, [CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "simulation_results[['QUERY_HASH1', 'DESCRIPTION1', f'{METRIC}1', f'{METRIC}2', f'{METRIC}r']].sort_values([f'{METRIC}r'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLUSTERING_COLUMN = \"l_orderkey\"\n",
    "aggregate_results = clustering.evaluation.evaluate_aggregates(m, TPCH[CLUSTERING_COLUMN], [CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "print(f\"There are {len(aggregate_results)} aggregates\")\n",
    "aggregate_results['e'] = aggregate_results['TOTAL_ERROR'] ** 2\n",
    "aggregate_results.sort_values(['e'], ascending=False)[['QUERY_HASH', 'DESCRIPTION1', 'RUNTIME_BASE_MS', 'RUNTIME_ESTIMATE_MS', 'RUNTIME_CLUSTERED_MS', 'TOTAL_ERROR_MS', 'RELATIVE_ERROR', 'RUNTIME_ESTIMATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_aggregate_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clustering.evaluation.print_aggregated_metrics(aggregate_results, m.query_frequencies)\n",
    "fig = clustering.evaluation.plot_aggregate_errors(aggregate_results, old_aggregate_results, m.query_frequencies)\n",
    "#old_aggregate_results = aggregate_results\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m.aggregates[(m.aggregates['GROUP_COLUMNS'] == 1) & (m.aggregates['AGGREGATE_COLUMNS'] == 1) & (m.aggregates['RUNTIME_NS'] > 500e6)]\n",
    "#m.aggregates[m.aggregates['COLUMN_NAME'] == 'l_orderkey,l_quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_latency_ms(df, m):\n",
    "    df['frequency'] = df['QUERY_HASH'].apply(lambda h: m.query_frequencies[h])\n",
    "    total_sum = df['frequency'] * df['RUNTIME_NS']\n",
    "    return total_sum.sum() / df['frequency'].sum() / 1e6\n",
    "\n",
    "def avg_latencies(table_name, column_name):\n",
    "    m2 = create_model(table_name, PQPInputParser(\"tpch\", TPCH[column_name]), 2)\n",
    "    m2.joins = m2.joins[(m2.joins['PROBE_TABLE'] == table_name) | (m2.joins['BUILD_TABLE'] == table_name)]\n",
    "    m2.aggregates = m2.aggregates[m2.aggregates['TABLE_NAME'].apply(lambda x: table_name in x)]\n",
    "\n",
    "    print(f\"average latency with {column_name} clustering:\")\n",
    "    print(f\"scans: {avg_latency_ms(m2.table_scans, m)}  ms\")\n",
    "    print(f\"joins: {avg_latency_ms(m2.joins, m)} ms\")\n",
    "    print(f\"aggregates: {avg_latency_ms(m2.aggregates, m)} ms\")\n",
    "    print()\n",
    "    \n",
    "avg_latencies(\"lineitem\", \"l_orderkey\")\n",
    "avg_latencies(\"lineitem\", \"l_shipdate\")\n",
    "avg_latencies(\"lineitem\", \"l_partkey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
