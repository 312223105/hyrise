{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import json\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "BENCHMARK = \"TPCH\"\n",
    "CHUNK_SIZE = 65535\n",
    "\n",
    "if BENCHMARK == \"TPCH\":\n",
    "    SCALE_FACTOR = 1\n",
    "    RUNS = 10\n",
    "    TIME = 5000\n",
    "    STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-H__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}__ENCODING_DictionaryFSBA\"\n",
    "    #STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPCH-BASE\"\n",
    "    #STATISTICS_PATH = \"~/Dokumente/repos/example_plugin/stats/final/tpch/sf10_r10/l_orderkey-100\"\n",
    "    \n",
    "elif BENCHMARK == \"TPCDS\":\n",
    "    SCALE_FACTOR = 10\n",
    "    RUNS = 1\n",
    "    TIME = 60\n",
    "    #STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/TPC-DS__SF_{SCALE_FACTOR}.000000__RUNS_{RUNS}__TIME_{TIME}__ENCODING_DictionaryFSBA\"\n",
    "    STATISTICS_PATH = \"~/Dokumente/repos/example_plugin/stats/final/tpcds/sf10-2d/nosort\"\n",
    "else:\n",
    "    raise Exception(\"Unknown benchmark: \" + BENCHMARK)\n",
    "\n",
    "print(f\"Model is configured for {BENCHMARK} (chunk size {CHUNK_SIZE}) with scale factor {SCALE_FACTOR}, {TIME} seconds runtime, and at most {RUNS} runs per query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table scan statistics\n",
    "\n",
    "path = f\"{STATISTICS_PATH}/table_scans.csv\"\n",
    "scans = pd.read_csv(path, sep='|')\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "LOADED_CHUNK_SIZE = CHUNK_SIZE\n",
    "LOADED_BENCHMARK = BENCHMARK\n",
    "LOADED_SCALE_FACTOR = SCALE_FACTOR\n",
    "LOADED_RUNS = RUNS\n",
    "LOADED_TIME = TIME\n",
    "\n",
    "print(f\"Successfully loaded {path}\")\n",
    "\n",
    "def assert_correct_statistics_loaded():\n",
    "    assert BENCHMARK == LOADED_BENCHMARK, f\"The model is configured to use {BENCHMARK}, but {LOADED_BENCHMARK} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert SCALE_FACTOR == LOADED_SCALE_FACTOR, f\"The model is configured to use {SCALE_FACTOR} as scale factor, but data for a scale factor of {LOADED_SCALE_FACTOR} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert RUNS == LOADED_RUNS, f\"The model is configured to perform at most {RUNS} runs, but the currently loaded data had at most {LOADED_RUNS} runs.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert TIME == LOADED_TIME, f\"The model is configured to run for {TIME} seconds, but the currently data had a runtime of {LOADED_TIME} seconds.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert CHUNK_SIZE == LOADED_CHUNK_SIZE, f\"The model is configured to use {CHUNK_SIZE} as chunk_size, but data for a chunk size of {LOADED_CHUNK_SIZE} is currently loaded.\\nEither change the benchmark or re-run all cells\"\n",
    "    assert EXPECTED_SCAN_COUNT == len(scans), f\"There should be {EXPECTED_SCAN_COUNT} table scans, but there are only {len(scans)}\\nProbably one of the last commands reassigned it unintentionally\"\n",
    "    \n",
    "    assert 'GET_TABLE_HASH' in scans.columns, f\"the statistics in {STATISTICS_PATH} are outdated (column 'GET_TABLE_HASH' in table_scans.csv is missing). Please create them again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "# To make sure pruning was not active,\n",
    "# first fetch table sizes,\n",
    "table_statistics = pd.read_csv(f\"{STATISTICS_PATH}/table_meta_data.csv\", sep='|')\n",
    "table_sizes = dict(zip(table_statistics.TABLE_NAME, table_statistics.ROW_COUNT))\n",
    "\n",
    "# then make sure INPUT_ROW_COUNT == table_size\n",
    "def input_size_matches(row):\n",
    "    #print(row)\n",
    "    \n",
    "    actual_row_count = row['INPUT_ROW_COUNT']\n",
    "    table = row['TABLE_NAME']\n",
    "    expected_row_count = table_sizes[table]\n",
    "    return expected_row_count == actual_row_count\n",
    "\n",
    "data_scans = scans[scans['COLUMN_TYPE'] == 'DATA']\n",
    "input_size_matches = data_scans.apply(input_size_matches, axis=1)\n",
    "all_sizes_match = reduce(np.logical_and, input_size_matches) #input_size_matches.apply()\n",
    "\n",
    "if not all_sizes_match:\n",
    "    #raise Exception(\"The given statistics were probably created while pruning was active\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"OK - looks like pruning was deactivated while the statistics were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append additional information to the table scans\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "print(f\"Statistics for {BENCHMARK} contain {len(scans)} table scans\")\n",
    "\n",
    "\n",
    "# Add statistics about selectivity and speed for each operator\n",
    "scans['selectivity'] = scans['OUTPUT_ROW_COUNT'] / scans['INPUT_ROW_COUNT']\n",
    "\n",
    "# TODO: Assumption that reading and writing a row have the same cost\n",
    "scans['time_per_row'] = scans['RUNTIME_NS'] / (scans['INPUT_ROW_COUNT'] + scans['OUTPUT_ROW_COUNT'])\n",
    "scans['time_per_input_row'] = scans['time_per_row']\n",
    "scans['time_per_output_row'] = scans['time_per_row']\n",
    "\n",
    "\n",
    "def determine_or_chains(table_scans):\n",
    "    table_scans['part_of_or_chain'] = False\n",
    "    \n",
    "    single_table_scans = table_scans.groupby(['QUERY_HASH', 'TABLE_NAME', 'GET_TABLE_HASH'])\n",
    "    \n",
    "    for _, scans in single_table_scans:\n",
    "        input_row_frequencies = Counter(scans.INPUT_ROW_COUNT)\n",
    "        or_input_sizes = set([input_size for input_size, frequency in input_row_frequencies.items() if frequency > 1])\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['INPUT_ROW_COUNT'] = scans['INPUT_ROW_COUNT']\n",
    "        df['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "        df['part_of_or_chain'] = scans.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        for _ in range(len(scans)):\n",
    "            or_input_sizes |= set(df[df['part_of_or_chain']].OUTPUT_ROW_COUNT.unique())\n",
    "            df['part_of_or_chain'] = df.apply(lambda row: row['INPUT_ROW_COUNT'] in or_input_sizes, axis=1)\n",
    "\n",
    "        or_chains = list(df[df['part_of_or_chain']].index)\n",
    "        #table_scans.iloc[or_chains, table_scans.columns.get_loc('part_of_or_chain')] = True\n",
    "        table_scans.loc[or_chains, 'part_of_or_chain'] = True\n",
    "    \n",
    "    return table_scans\n",
    "\n",
    "# Hyrise does not use scans that are part of an OR-chain for pruning\n",
    "scans = determine_or_chains(scans)\n",
    "\n",
    "\n",
    "# Like scans are not useful if they start with %\n",
    "# TODO what if they dont start with % and contain more than one % ? -> up to first % prunable, but is it used?\n",
    "def benefits_from_sorting(row):    \n",
    "    description = row['DESCRIPTION']\n",
    "    if \"ColumnLike\" in description:\n",
    "        words = description.split('LIKE')\n",
    "        assert len(words) == 2, f\"expected exactly one occurence of LIKE, but got {description}\"\n",
    "        like_criteria = words[1]\n",
    "        assert \"%\" in like_criteria or \"_\" in like_criteria, f\"LIKE operators should have an % or _, but found none in {like_criteria}\"\n",
    "        first_char = like_criteria[2]\n",
    "        assert first_char != ' ' and first_char != \"'\", \"Like check considers the wrong token\"\n",
    "        return first_char != '%' and first_char != '_'\n",
    "    elif \"ExpressionEvaluator\" in description and \" IN \" in description:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "scans['benefits_from_sorting'] = scans.apply(benefits_from_sorting, axis=1)\n",
    "# TODO: valid atm, but feels a bit hacky to assume not benefitting from sorted segments -> not benefitting from pruning\n",
    "scans['useful_for_pruning'] = scans.apply(lambda row: not row['part_of_or_chain'] and row['benefits_from_sorting'] , axis=1)\n",
    "EXPECTED_SCAN_COUNT = len(scans)\n",
    "print(f\"Of those, only {len(scans[scans['useful_for_pruning']])} are useful for pruning\")\n",
    "\n",
    "print(\"TODO: For now, filtering on scans is deactivated. This is because all scans are needed to recognize OR-Chains. Models have to take care themselves whether a scan can contribute to pruning or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_determine_or_chains():\n",
    "    test = pd.DataFrame()\n",
    "    test['QUERY_HASH'] = pd.Series(['1']*3  + ['2']*4)\n",
    "    test['TABLE_NAME'] = pd.Series(['lineitem']*3  + ['part']*4)\n",
    "    test['GET_TABLE_HASH'] = pd.Series(['0x1'] + ['0x2']*2 + ['0x3']*4)\n",
    "    test['COLUMN_NAME'] = pd.Series(['l_shipdate', 'l_shipdate', 'l_discount', 'p_brand', 'p_type', 'p_type', 'p_size'])\n",
    "    test['INPUT_ROW_COUNT'] = pd.Series( [6001215, 6001215, 200000, 200000, 199000, 199000, 50000])\n",
    "    test['OUTPUT_ROW_COUNT'] = pd.Series([ 400000,  300000, 200000, 199000,      0,  50000, 20000])\n",
    "    test_result = determine_or_chains(test)\n",
    "    assert len(test_result) == 7, \"should not filter out any rows\"    \n",
    "    assert len(test_result[test_result['part_of_or_chain']]) == 3, \"expected 3 scans, got\\n\" + str(test_result)\n",
    "    assert list(test_result['part_of_or_chain']) == [False]*4 + [True]*3\n",
    "    print(\"Test OK\")\n",
    "\n",
    "test_determine_or_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query frequency information\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def get_query_frequencies():\n",
    "    plan_cache = pd.read_csv(f\"{STATISTICS_PATH}/plan_cache.csv\", sep='|')\n",
    "    return dict(zip(plan_cache.QUERY_HASH, plan_cache.EXECUTION_COUNT))\n",
    "\n",
    "#get_query_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column statistics - especially interesting: number of distinct values, and columns sorted during statistics creation\n",
    "\n",
    "# Returns a 2-level-dictionary: distinct_values[TABLE][COLUMN] = number_of_distinct_values\n",
    "def get_distinct_values_count():        \n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    column_statistics_df['DISTINCT_VALUES'] = np.int32(column_statistics_df['DISTINCT_VALUES'])\n",
    "    tables_and_columns = column_statistics_df.groupby('TABLE_NAME')\n",
    "    distinct_values = {table: dict(zip(column_df.COLUMN_NAME, column_df.DISTINCT_VALUES)) for table, column_df in tables_and_columns }\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    num_tables = len(distinct_values)\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        assert num_tables == 8, f\"TPCH has 8 tables, but got {num_tables}\"\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        assert num_tables == 24, f\"TPCDS has 24 tables, but got {num_tables}\"\n",
    "    else:\n",
    "        assert False, \"Insert a benchmark specific check here\"\n",
    "    \n",
    "    return distinct_values\n",
    "\n",
    "# Returns a dictionary: sorted_columns_during_creation[TABLE] = [column1, column2, ...]\n",
    "def get_sorted_columns_during_creation():\n",
    "    # Code\n",
    "    column_statistics_df = pd.read_csv(f\"{STATISTICS_PATH}/column_meta_data.csv\", sep='|')\n",
    "    globally_sorted_columns = column_statistics_df[column_statistics_df['IS_GLOBALLY_SORTED'] == 1]\n",
    "    \n",
    "    tables_and_columns = globally_sorted_columns.groupby('TABLE_NAME')\n",
    "    globally_sorted_columns = {table: list(column_df.COLUMN_NAME) for table, column_df in tables_and_columns }\n",
    "    \n",
    "    return globally_sorted_columns\n",
    "\n",
    "#get_distinct_values_count()\n",
    "#get_sorted_columns_during_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JOINS ###\n",
    "\n",
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def load_join_statistics(path=STATISTICS_PATH):\n",
    "    def line_looks_suspicious(row):\n",
    "        right_table_name = row['RIGHT_TABLE_NAME']    \n",
    "        if pd.isnull(right_table_name):\n",
    "            pass\n",
    "        elif row['RIGHT_TABLE_ROW_COUNT'] > table_sizes[row['RIGHT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        left_table_name = row['LEFT_TABLE_NAME']\n",
    "        if pd.isnull(left_table_name):\n",
    "            pass\n",
    "        elif row['LEFT_TABLE_ROW_COUNT'] > table_sizes[row['LEFT_TABLE_NAME']]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def validate_joins(joins):\n",
    "        is_suspicious = joins.apply(line_looks_suspicious, axis=1)\n",
    "        suspicious_joins = joins[is_suspicious]\n",
    "        assert len(suspicious_joins) < 4, f\"there are {len(suspicious_joins)} suspicious joins:\\n{suspicious_joins[['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]}\"\n",
    "    \n",
    "    joins = pd.read_csv(f\"{path}/joins.csv\", sep='|')\n",
    "    joins = joins.dropna()\n",
    "    joins['PROBE_TABLE'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_COLUMN'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_COLUMN_NAME\"] if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['PROBE_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_ROW_COUNT\"]if not x['PROBE_SIDE'] == \"NULL\" else \"NULL\" , axis=1)\n",
    "    joins['BUILD_TABLE'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_COLUMN'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_COLUMN_NAME\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    joins['BUILD_TABLE_ROW_COUNT'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_ROW_COUNT\"] if not x['BUILD_SIDE'] == \"NULL\" else \"NULL\", axis=1)\n",
    "    validate_joins(joins)    \n",
    "                                                                                           \n",
    "    tmp = joins['BUILD_SORTED'].copy()\n",
    "    joins['BUILD_SORTED'] = joins['PROBE_SORTED'].copy()\n",
    "    joins['PROBE_SORTED'] = tmp\n",
    "    \n",
    "    return joins\n",
    "\n",
    "load_join_statistics()#.iloc[50:60][['JOIN_MODE', 'LEFT_TABLE_NAME', 'LEFT_COLUMN_NAME', 'LEFT_TABLE_ROW_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_COLUMN_NAME', 'RIGHT_TABLE_ROW_COUNT', 'OUTPUT_ROW_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract interesting table names\n",
    "# Currently fixed to columns which are scanned or used in joins\n",
    "\n",
    "def get_table_names(table_scans, joins):\n",
    "    scan_tables = set(table_scans['TABLE_NAME'].unique())\n",
    "    left_join_tables = set(joins['LEFT_TABLE_NAME'].unique())\n",
    "    right_join_tables = set(joins['RIGHT_TABLE_NAME'].unique())    \n",
    "\n",
    "    return scan_tables.union(left_join_tables.union(right_join_tables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel:\n",
    "    \n",
    "    def __init__(self, query_frequencies, table_name, table_scans, correlations={}):\n",
    "        self.query_frequencies = query_frequencies\n",
    "        self.table_name = table_name\n",
    "        self.table_scans = table_scans\n",
    "        self.correlations = correlations\n",
    "        \n",
    "    def query_frequency(self, query_hash):\n",
    "        return self.query_frequencies[query_hash]\n",
    "        \n",
    "    def extract_scan_columns(self):\n",
    "        useful_scans = self.table_scans[self.table_scans['useful_for_pruning']]\n",
    "        interesting_scan_columns = list(useful_scans['COLUMN_NAME'].unique())\n",
    "        \n",
    "        return interesting_scan_columns\n",
    "    \n",
    "    def extract_join_columns(self):\n",
    "        interesting_join_probe_columns = list(self.joins[self.joins['PROBE_TABLE'] == self.table_name]['PROBE_COLUMN'].unique())\n",
    "        interesting_join_build_columns = list(self.joins[self.joins['BUILD_TABLE'] == self.table_name]['BUILD_COLUMN'].unique())        \n",
    "        \n",
    "        return self.uniquify(interesting_join_probe_columns + interesting_join_build_columns)\n",
    "    \n",
    "    def extract_interesting_columns(self):        \n",
    "        return self.uniquify(self.extract_scan_columns() + self.extract_join_columns())\n",
    "    \n",
    "    def round_up_to_next_multiple(self, number_to_round, base_for_multiple):\n",
    "        quotient = number_to_round // base_for_multiple\n",
    "        if number_to_round % base_for_multiple != 0:\n",
    "            quotient += 1\n",
    "        return quotient * base_for_multiple        \n",
    "\n",
    "    def uniquify(self, seq):\n",
    "            seen = set()\n",
    "            return [x for x in seq if not (x in seen or seen.add(x))]    \n",
    "    \n",
    "    # return a list of possible clusterings\n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTableMdcModel(AbstractModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(query_frequencies, table_name, table_scans, correlations)\n",
    "        self.max_dimensions = max_dimensions\n",
    "        self.table_sizes = table_sizes       \n",
    "        self.table_size = table_sizes[table_name]\n",
    "        self.distinct_values = distinct_values\n",
    "        self.target_chunksize = target_chunksize\n",
    "        self.joins = joins\n",
    "        self.sorted_columns_during_creation = sorted_columns_during_creation\n",
    "        \n",
    "        \n",
    "        self.join_column_names = self.extract_join_columns()\n",
    "        self.scan_column_names = self.extract_scan_columns()\n",
    "        \n",
    "        self.scan_estimates = pd.DataFrame()\n",
    "        self.scan_estimates['QUERY_HASH'] = self.table_scans['QUERY_HASH']        \n",
    "        self.scan_estimates['DESCRIPTION'] = self.table_scans['DESCRIPTION']\n",
    "        self.scan_estimates['RUNTIME_ESTIMATE'] = np.array([-1] * len(self.table_scans))\n",
    "        self.scan_estimates['RUNTIME_NS'] = self.table_scans['RUNTIME_NS'] # TODO: probably not necessary?\n",
    "        self.scan_estimates['time_per_input_row'] = self.table_scans['time_per_input_row']\n",
    "        #self.scan_estimates.index = self.table_scans.index\n",
    "        \n",
    "        self.join_estimates = pd.DataFrame()\n",
    "        self.join_estimates['QUERY_HASH'] = self.joins['QUERY_HASH']\n",
    "        self.join_estimates['DESCRIPTION'] = self.joins['DESCRIPTION']\n",
    "        self.join_estimates['ESTIMATE_BUILD_SIDE_MATERIALIZING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['ESTIMATE_PROBE_SIDE_MATERIALIZING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['ESTIMATE_CLUSTERING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['ESTIMATE_BUILDING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['ESTIMATE_PROBING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['ESTIMATE_OUTPUT_WRITING'] = np.array([-1] * len(self.joins))\n",
    "        self.join_estimates['BUILD_SIDE_MATERIALIZING_NS'] = self.joins['BUILD_SIDE_MATERIALIZING_NS']\n",
    "        self.join_estimates['PROBE_SIDE_MATERIALIZING_NS'] = self.joins['PROBE_SIDE_MATERIALIZING_NS']\n",
    "        self.join_estimates['CLUSTERING_NS'] = self.joins['CLUSTERING_NS']\n",
    "        self.join_estimates['BUILDING_NS'] = self.joins['BUILDING_NS']\n",
    "        self.join_estimates['PROBING_NS'] = self.joins['PROBING_NS']\n",
    "        self.join_estimates['OUTPUT_WRITING_NS'] = self.joins['OUTPUT_WRITING_NS']\n",
    "        self.join_estimates['RUNTIME_NS'] = self.joins['RUNTIME_NS']\n",
    "        \n",
    "    def is_join_column(self, column_name):\n",
    "        return column_name in self.join_column_names\n",
    "    \n",
    "    def is_scan_column(self, column_name):\n",
    "        return column_name in self.scan_column_names\n",
    "    \n",
    "    def suggest_clustering(self, first_k=1):\n",
    "        interesting_columns = self.extract_interesting_columns()\n",
    "\n",
    "        print(interesting_columns)\n",
    "        clustering_columns = itertools.combinations_with_replacement(interesting_columns, self.max_dimensions)\n",
    "        clustering_columns = [self.uniquify(clustering) for clustering in clustering_columns]\n",
    "        print(f\"There are {len(clustering_columns)} clustering column sets\")\n",
    "        \n",
    "        reduced_clustering_columns = []\n",
    "        seen_clusterings = set()\n",
    "        for clustering in clustering_columns:\n",
    "            clustering_string = \"-\".join(clustering)\n",
    "            if clustering_string not in seen_clusterings:\n",
    "                seen_clusterings.add(clustering_string)\n",
    "                reduced_clustering_columns.append(clustering)\n",
    "        clustering_columns = reduced_clustering_columns\n",
    "        \n",
    "        \n",
    "        print(f\"{len(clustering_columns)} of them are unique\")\n",
    "        sort_columns = interesting_columns        \n",
    "        clusterings_with_runtimes = reduce(lambda x,y: x+y,[self.estimate_total_runtimes(clustering_cols, sort_columns) for clustering_cols in clustering_columns])\n",
    "        clusterings_with_runtimes.sort(key=lambda x: x[2], reverse=False)\n",
    "        \n",
    "        return clusterings_with_runtimes[0:first_k]\n",
    "    \n",
    "    \n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Each model should provide this function\")\n",
    "        \n",
    "    def estimate_distinct_values_per_chunk_at_statistics_time(self, column, table):        \n",
    "        if column in self.sorted_columns_during_creation.get(table, {}):\n",
    "            # Column was globally sorted\n",
    "            average_count_per_distinct_value = self.table_sizes[table] / self.distinct_values[table][column]\n",
    "            return math.ceil(self.target_chunksize / average_count_per_distinct_value)\n",
    "        else:\n",
    "            # Column was not globally sorted\n",
    "            total_distinct_values = self.distinct_values[table][column]\n",
    "            return min(total_distinct_values, self.target_chunksize)        \n",
    "    \n",
    "    def compute_unprunable_parts(self, row, clustering_columns, split_factors):\n",
    "        def clustering_columns_correlated_to(column):\n",
    "            return [clustering_column for clustering_column in clustering_columns if column in self.correlations.get(clustering_column, {})]\n",
    "\n",
    "        def correlates_to_clustering_column(column):\n",
    "            return len(clustering_columns_correlated_to(column)) > 0\n",
    "\n",
    "        column_name = row['COLUMN_NAME']\n",
    "\n",
    "        if not row['useful_for_pruning']:\n",
    "            selectivity = 1\n",
    "        elif column_name in clustering_columns:\n",
    "            scan_selectivity = row['selectivity']\n",
    "            split_factor = split_factors[clustering_columns.index(column_name)]\n",
    "            selectivity =  self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor)\n",
    "        elif correlates_to_clustering_column(column_name):\n",
    "            scan_selectivity = row['selectivity']\n",
    "            correlated_clustering_columns = clustering_columns_correlated_to(column_name)\n",
    "\n",
    "            # ToDo this is hacky, but for now assume there is just one correlated column\n",
    "            assert len(correlated_clustering_columns) == 1, f\"expected just 1 correlated clustering column, but got {len(correlated_clustering_columns)}\"\n",
    "\n",
    "            split_factor = split_factors[clustering_columns.index(correlated_clustering_columns[0])]\n",
    "            selectivity = min(1, 1.2 * self.round_up_to_next_multiple(scan_selectivity, 1 / split_factor))\n",
    "        else:\n",
    "            selectivity = 1\n",
    "\n",
    "        return selectivity\n",
    "    \n",
    "    def estimate_table_scan_runtime(self, clustering_columns, sorting_column, split_factors):                \n",
    "        def compute_tablescan_runtime(row, sorting_column):\n",
    "            assert row['estimated_input_rows'] > 1, row\n",
    "            assert row['runtime_per_input_row'] > 0, row\n",
    "            assert row['runtime_per_output_row'] > 0, row\n",
    "            input_row_count = row['estimated_input_rows']\n",
    "            \n",
    "            if row['COLUMN_NAME'] == sorting_column and row['benefits_from_sorting'] and not row['COLUMN_NAME'] in self.sorted_columns_during_creation.get(self.table_name, {}):\n",
    "                # TODO is this the best way to simulate sorted access?\n",
    "                input_row_count = np.log2(input_row_count)\n",
    "\n",
    "            runtime = input_row_count * row['runtime_per_input_row'] + row['OUTPUT_ROW_COUNT'] * row['runtime_per_output_row']\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "        \n",
    "        original_clustering_column = self.sorted_columns_during_creation[self.table_name][0]\n",
    "        \n",
    "        \n",
    "        runtime = 0\n",
    "        scans_per_query = self.table_scans.sort_values(['INPUT_ROW_COUNT'], ascending=False).groupby(['QUERY_HASH', 'GET_TABLE_HASH'])\n",
    "        for _, scans in scans_per_query:\n",
    "            number_of_scans = len(scans)\n",
    "            assert number_of_scans > 0 and number_of_scans < 25, f\"weird scan length: {number_of_scans}\\nScans:\\n{scans}\"\n",
    "            # TODO: kinda unrealistic assumption: everything not in the table scan result can be pruned            \n",
    "\n",
    "            unprunable_parts = scans.apply(self.compute_unprunable_parts, axis=1, args=(clustering_columns, split_factors,))\n",
    "            unprunable_part = unprunable_parts.product()\n",
    "            assert unprunable_part > 0, \"no unprunable part\"\n",
    "            \n",
    "            estimated_pruned_table_size = min(self.table_size, self.round_up_to_next_multiple(unprunable_part * self.table_size, CHUNK_SIZE))\n",
    "            \n",
    "            runtimes = pd.DataFrame()\n",
    "            runtimes['QUERY_HASH'] = scans['QUERY_HASH']\n",
    "            runtimes['runtime_per_input_row'] = scans['time_per_input_row']\n",
    "            runtimes['runtime_per_output_row'] = scans['time_per_output_row']\n",
    "            runtimes['COLUMN_NAME'] = scans['COLUMN_NAME']\n",
    "            runtimes['benefits_from_sorting'] = scans['benefits_from_sorting']\n",
    "            # the pruned table inputs should be reflected in 'estimated_input_rows'\n",
    "            runtimes['estimated_input_rows'] = scans['INPUT_ROW_COUNT']\n",
    "            runtimes['OUTPUT_ROW_COUNT'] = scans['OUTPUT_ROW_COUNT']\n",
    "\n",
    "            runtimes.iloc[0, runtimes.columns.get_loc('estimated_input_rows')] = estimated_pruned_table_size\n",
    "            assert runtimes['estimated_input_rows'].iloc[0] == estimated_pruned_table_size, f\"value is {runtimes.iloc[0]['estimated_input_rows']}, but should be {estimated_pruned_table_size}\"\n",
    "            # TODO modify input sizes of subsequent scans\n",
    "            \n",
    "            scan_runtimes = runtimes.apply(compute_tablescan_runtime, axis=1, args=(sorting_column,))\n",
    "            #self.table_scans.iloc[:, self.table_scans.columns.get_loc('RUNTIME_ESTIMATE')] = scan_runtimes\n",
    "            self.scan_estimates.loc[scan_runtimes.index, 'RUNTIME_ESTIMATE'] = scan_runtimes\n",
    "            runtime += scan_runtimes.sum()\n",
    "        return runtime\n",
    "\n",
    "    def estimate_chunk_count(self, scans, clustering_columns, dimension_cardinalities):\n",
    "        raise NotImplementedError(\"Subclass responsibility\")\n",
    "    \n",
    "    def get_chunk_count_factor(self, row, side, clustering_columns, dimension_cardinalities):\n",
    "        query_hash = row['QUERY_HASH']\n",
    "        if side == \"PROBE\":\n",
    "            input_rows = row['PROBE_TABLE_ROW_COUNT']\n",
    "            assert row['PROBE_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            \n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        elif side == \"BUILD\":\n",
    "            input_rows = row['BUILD_TABLE_ROW_COUNT']\n",
    "            assert row['BUILD_TABLE'] == self.table_name, \"Call this function only for the own table\"\n",
    "            # When joining tables, the table size might increase (a lot). This makes it hard to estimate the chunk count, so just ignore it\n",
    "            if input_rows > self.table_size:\n",
    "                return 1\n",
    "        else:\n",
    "            raise ValueError(\"side must be PROBE or BUILD\")\n",
    "\n",
    "        expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, clustering_columns, dimension_cardinalities)\n",
    "        \n",
    "        min_chunk_count = math.ceil(input_rows / self.target_chunksize)\n",
    "        max_chunk_count = math.ceil(self.table_size / self.target_chunksize)        \n",
    "\n",
    "        CHUNK_COUNT_SPEEDUP_LOW = 3\n",
    "        CHUNK_COUNT_SPEEDUP_HIGH = 1\n",
    "\n",
    "        current_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (expected_chunk_count - min_chunk_count) / (max_chunk_count + 0.01 - min_chunk_count))\n",
    "        #print(f\"current speedup: {current_speedup}\")\n",
    "\n",
    "        old_clustering_columns = self.sorted_columns_during_creation[self.table_name]\n",
    "        old_dimension_cardinalities = [self.statistic_time_dimension_cardinalities()] * len(old_clustering_columns)\n",
    "        old_expected_chunk_count = self.estimate_chunk_count(query_hash, input_rows, old_clustering_columns, old_dimension_cardinalities)\n",
    "        old_speedup = self.interpolate(CHUNK_COUNT_SPEEDUP_LOW, CHUNK_COUNT_SPEEDUP_HIGH, (old_expected_chunk_count - min_chunk_count) / (max_chunk_count + 0.01 - min_chunk_count))\n",
    "        #print(f\"old speedup: {old_speedup}\")\n",
    "\n",
    "        return 1 * old_speedup / current_speedup\n",
    "    \n",
    "    def estimate_join_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        def compute_join_runtime(row, sorting_column):\n",
    "            if \"JoinHash\" in row['DESCRIPTION']:\n",
    "                probe_column = row['PROBE_COLUMN']\n",
    "                if row['PROBE_TABLE'] == self.table_name:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    probe_column_is_sorted = row['PROBE_SORTED'] and probe_column == sorting_column\n",
    "                    materialize_probe_factor = self.get_chunk_count_factor(row, \"PROBE\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    probe_column_was_sorted = row['PROBE_SORTED'] and probe_column in self.sorted_columns_during_creation.get(row['PROBE_TABLE'], {})\n",
    "                    probe_column_is_sorted = probe_column_was_sorted\n",
    "                    materialize_probe_factor = 1\n",
    "                    \n",
    "                build_column = row['BUILD_COLUMN']\n",
    "                if row['BUILD_TABLE'] == self.table_name:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(self.table_name, {})\n",
    "                    build_column_is_sorted = row['BUILD_SORTED'] and build_column == sorting_column\n",
    "                    materialize_build_factor = self.get_chunk_count_factor(row, \"BUILD\", clustering_columns, dimension_cardinalities)\n",
    "                else:\n",
    "                    build_column_was_sorted = row['BUILD_SORTED'] and build_column in self.sorted_columns_during_creation.get(row['BUILD_TABLE'], {})\n",
    "                    build_column_is_sorted = build_column_was_sorted\n",
    "                    materialize_build_factor = 1\n",
    "\n",
    "                time_materialize_probe = row['PROBE_SIDE_MATERIALIZING_NS']\n",
    "                time_materialize_build = row['BUILD_SIDE_MATERIALIZING_NS']                \n",
    "                \n",
    "                def get_materialize_factor(column, was_globally_sorted, is_sorted, expected_distinct_value_count):\n",
    "                    # Assumption: \"was_sorted\" implies global sortedness, i.e., both clustering and chunkwise sorting\n",
    "                    # This is true when the clustering produced by the table generator is used by the plan cache exporter\n",
    "                    # If the data has been re-clustered before the plan cache exporter runs, there has to be some system inside Hyrise which tracks the current clustering config\n",
    "                    \n",
    "                    # Sortedness seems to yield a speed up of approx. 1.6, regardless of the number of distinct values\n",
    "                    SORT_SPEEDUP = 1.6\n",
    "                    sortedness_factor = 1                    \n",
    "                    was_sorted = was_globally_sorted\n",
    "                    if was_sorted:\n",
    "                        sortedness_factor *= SORT_SPEEDUP\n",
    "                    if is_sorted:\n",
    "                        sortedness_factor /= SORT_SPEEDUP\n",
    "                    \n",
    "\n",
    "                    # The influence of clustering depends on the number of distinct values\n",
    "                    CLUSTERING_SPEEDUP_LOW = 1.84\n",
    "                    CLUSTERING_SPEEDUP_HIGH = 1\n",
    "                    clustering_factor = 1\n",
    "                    statistics_time_distinct_value_count = self.estimate_distinct_values_per_chunk_at_statistics_time(column, self.table_name);\n",
    "                    clustering_factor *= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, statistics_time_distinct_value_count / self.target_chunksize)\n",
    "                    clustering_factor /= self.interpolate(CLUSTERING_SPEEDUP_LOW, CLUSTERING_SPEEDUP_HIGH, expected_distinct_value_count / self.target_chunksize)\n",
    "                        \n",
    "                    return sortedness_factor * clustering_factor                \n",
    "                \n",
    "                if row['PROBE_TABLE'] == self.table_name and row['PROBE_SORTED']:\n",
    "                    expected_distinct_values_probe = self.estimate_distinct_values_per_chunk(probe_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_probe_factor *= get_materialize_factor(probe_column, probe_column_was_sorted, probe_column_is_sorted, expected_distinct_values_probe)\n",
    "                    \n",
    "                if row['BUILD_TABLE'] == self.table_name and row['BUILD_SORTED']:\n",
    "                    expected_distinct_values_build = self.estimate_distinct_values_per_chunk(build_column, clustering_columns, sorting_column, dimension_cardinalities)\n",
    "                    materialize_build_factor *= get_materialize_factor(build_column, build_column_was_sorted, build_column_is_sorted, expected_distinct_values_build)\n",
    "                \n",
    "                time_materialize = time_materialize_probe * materialize_probe_factor + time_materialize_build *  materialize_build_factor\n",
    "\n",
    "                #print(f\"row.name is {row.name}\")\n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_BUILD_SIDE_MATERIALIZING'] =  time_materialize_build *  materialize_build_factor\n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_PROBE_SIDE_MATERIALIZING'] =  time_materialize_probe *  materialize_probe_factor\n",
    "\n",
    "                # unchanged\n",
    "                time_cluster = row['CLUSTERING_NS']\n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_CLUSTERING'] = time_cluster\n",
    "\n",
    "                # unchanged\n",
    "                time_build = row['BUILDING_NS']\n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_BUILDING'] =  time_build\n",
    "\n",
    "\n",
    "                time_probe = row['PROBING_NS']\n",
    "                probe_factor = 1\n",
    "                if probe_column_is_sorted:\n",
    "                    if not probe_column_was_sorted:\n",
    "                        probe_factor = 0.7\n",
    "                    else:\n",
    "                        probe_factor = 1\n",
    "                else:\n",
    "                    if probe_column_was_sorted:\n",
    "                        probe_factor = 1.3\n",
    "                    else:\n",
    "                        probe_factor = 1\n",
    "                \n",
    "                #elif probe_column_is_sorted or probe_column_is_clustered:\n",
    "                #    if not probe_column_was_sorted:\n",
    "                #        probe_factor = 0.9\n",
    "                #    else:\n",
    "                #        probe_factor = 1.1\n",
    "                #elif probe_column_was_sorted:\n",
    "                #    # probe column is now neither sorted nor clustered\n",
    "                #    probe_factor = 1.4\n",
    "\n",
    "                time_probe *= probe_factor                \n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_PROBING'] =  time_probe\n",
    "\n",
    "                # unchanged\n",
    "                time_write_output = row['OUTPUT_WRITING_NS']\n",
    "                self.join_estimates.loc[row.name, 'ESTIMATE_OUTPUT_WRITING'] =  time_write_output\n",
    "\n",
    "\n",
    "\n",
    "                # TODO: how to deal with the difference between RUNTIME_NS and sum(stage_runtimes)?\n",
    "                runtime = time_materialize + time_cluster + time_build + time_probe + time_write_output\n",
    "            else:\n",
    "                runtime = row['RUNTIME_NS']\n",
    "\n",
    "            return runtime * self.query_frequency(row['QUERY_HASH'])\n",
    "\n",
    "        join_runtimes = self.joins.apply(compute_join_runtime, axis=1, args=(sorting_column,))\n",
    "        return join_runtimes.sum()\n",
    "\n",
    "    def effective_clustering(self, clustering_columns, sort_column, cardinalities):\n",
    "        expected_num_clusters = np.prod(cardinalities)\n",
    "        chunks_in_table = math.ceil(self.table_size / self.target_chunksize)\n",
    "        expected_chunks_per_cluster = chunks_in_table / expected_num_clusters\n",
    "        #if expected_chunks_per_cluster < 1:\n",
    "        #    print(f\"WARNING: expected chunks per cluster is below 1: {expected_chunks_per_cluster}\")\n",
    "\n",
    "        if round(expected_chunks_per_cluster) < 2:\n",
    "            return clustering_columns, cardinalities\n",
    "        \n",
    "        columns = clustering_columns.copy()\n",
    "        counts = cardinalities.copy()\n",
    "        if sort_column not in columns:\n",
    "            columns.append(sort_column)\n",
    "            counts.append(1)\n",
    "            \n",
    "        counts[columns.index(sort_column)] = round(counts[columns.index(sort_column)] * expected_chunks_per_cluster)\n",
    "        assert len(counts) == len(columns)\n",
    "        \n",
    "        #print(f\"effective columns changed from {clustering_columns} to {columns}\")\n",
    "        #print(f\"effective counts changed from {cardinalities} to {counts}\")\n",
    "        \n",
    "        return columns, counts\n",
    "            \n",
    "    \n",
    "    def estimate_total_runtimes(self, clustering_columns, sorting_columns):\n",
    "        #print(f\"testing clustering {clustering_columns} with sorting columns {sorting_columns}\")\n",
    "        dimension_cardinalities = self.get_dimension_cardinalities(clustering_columns)\n",
    "        total_runtimes = {sorting_column: 0 for sorting_column in sorting_columns}\n",
    "        clusterings = []\n",
    "        for sorting_column in sorting_columns:\n",
    "            runtime = self.estimate_total_runtime(clustering_columns, sorting_column, dimension_cardinalities)\n",
    "            clusterings.append([list(zip(clustering_columns, dimension_cardinalities)), sorting_column, runtime])\n",
    "        \n",
    "        #clusterings = [[list(zip(clustering_columns, dimension_cardinalities)), sorting_column, np.int64(total_runtimes[sorting_column])] for sorting_column in sorting_columns]\n",
    "        return clusterings\n",
    "    \n",
    "    def estimate_total_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        effective_clustering_columns, effective_dimension_cardinalities = self.effective_clustering(clustering_columns, sorting_column, dimension_cardinalities)                    \n",
    "        \n",
    "        runtime = 0\n",
    "        runtime += self.estimate_table_scan_runtime(effective_clustering_columns, sorting_column, effective_dimension_cardinalities)\n",
    "        runtime += self.estimate_join_runtime(effective_clustering_columns, sorting_column, effective_dimension_cardinalities)\n",
    "        \n",
    "        return runtime\n",
    "    \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        raise NotImplementedError(\"Subclasses must override this function\")\n",
    "        \n",
    "    def interpolate(self, low, high, percentage):\n",
    "        assert percentage >= 0 and percentage <= 1, f\"percentage must between 0 and 1, but is {percentage}\"\n",
    "        return (1 - percentage) * low + (percentage * high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisjointClustersModel(SingleTableMdcModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation):\n",
    "        super().__init__(max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_distinct_values_per_chunk(self, column, clustering_columns, chunk_sorting_column, dimension_cardinalities):\n",
    "        total_distinct_values = self.distinct_values[self.table_name][column]\n",
    "        \n",
    "        if column in clustering_columns:\n",
    "            index = clustering_columns.index(column)\n",
    "            clusters_for_column = dimension_cardinalities[index]\n",
    "            return min(self.target_chunksize, total_distinct_values / clusters_for_column)\n",
    "        else:\n",
    "            # TODO cluster wise sorting\n",
    "            return min(self.target_chunksize, total_distinct_values)\n",
    "        \n",
    "    # This function should only be called for the own table, not for others\n",
    "    def estimate_chunk_count(self, query_hash, join_input_rows, clustering_columns, dimension_cardinalities):        \n",
    "        # TODO include or exclude scans that do not benefit from pruning? Excluded for now        \n",
    "        table_scans = self.table_scans[self.table_scans['QUERY_HASH'] == query_hash]        \n",
    "        table_scans = table_scans[table_scans['useful_for_pruning']]\n",
    "        \n",
    "        if len(table_scans) > 0:\n",
    "            #print(f\"For query hash {query_hash}, there are {len(table_scans)} scans on {self.table_name}\")\n",
    "            def get_denseness_factor(row):\n",
    "                column = row['COLUMN_NAME']\n",
    "                if column in clustering_columns:\n",
    "                    # TODO more precise estimate\n",
    "                    denseness_factor = 1\n",
    "                else:\n",
    "                    denseness_factor = row['selectivity']\n",
    "\n",
    "                return denseness_factor\n",
    "\n",
    "            denseness_factors = table_scans.apply(get_denseness_factor, axis=1)\n",
    "            denseness_factor = denseness_factors.product()\n",
    "            #print(f\"denseness factor is {denseness_factor}\")\n",
    "        else:\n",
    "            denseness_factor = 1        \n",
    "        \n",
    "        chunk_count = math.ceil(join_input_rows / (self.target_chunksize * denseness_factor))\n",
    "        max_chunks = math.ceil(self.table_size / self.target_chunksize)\n",
    "        #if chunk_count > max_chunks:\n",
    "        #    print(f\"WARNING: estimated {chunk_count} chunks, but {self.table_name} got only {max_chunks}\\nDenseness: {denseness_factor}\")\n",
    "        \n",
    "        \n",
    "        return min(chunk_count, max_chunks)\n",
    "    \n",
    "    def statistic_time_dimension_cardinalities(self):\n",
    "        return math.ceil(self.table_size / self.target_chunksize)\n",
    "    \n",
    "    def get_dimension_cardinalities(self, clustering_columns):\n",
    "        MAX_HISTOGRAM_BINS = 100\n",
    "        target_cluster_count = math.ceil(self.table_size / self.target_chunksize)\n",
    "    \n",
    "        join_columns = list(filter(lambda x: self.is_join_column(x), clustering_columns))\n",
    "        scan_columns = list(filter(lambda x: self.is_scan_column(x), clustering_columns))\n",
    "        intersecting_columns = set(join_columns).intersection(set(scan_columns))\n",
    "        assert len(intersecting_columns) == 0, f\"The following columns are used as both join and scan column: {intersecting_columns}\"\n",
    "        \n",
    "        #scan_columns = clustering_columns.copy()\n",
    "        #join_columns = []\n",
    "        \n",
    "        if len(scan_columns) == 0:\n",
    "            CLUSTERS_PER_JOIN_COLUMN = math.ceil(math.pow(target_cluster_count, 1/len(join_columns)))\n",
    "        else: \n",
    "            CLUSTERS_PER_JOIN_COLUMN = 3;\n",
    "        # Assumption: uniform distribution (in the sense that every cluster actually exists)\n",
    "        num_join_clusters = math.pow(CLUSTERS_PER_JOIN_COLUMN, len(join_columns))\n",
    "        assert num_join_clusters <= 2 * target_cluster_count, f\"Would get {num_join_clusters} clusters for join columns, but aimed at at most {target_cluster_count} clusters\"\n",
    "    \n",
    "        # only applies to scan columns\n",
    "        desired_scan_clusters_count = math.ceil(target_cluster_count / num_join_clusters)\n",
    "        individual_distinct_values = [self.distinct_values[self.table_name][column] for column in scan_columns]\n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "\n",
    "        global_modification_factor = desired_scan_clusters_count / log_distinct_values_product\n",
    "        num_scan_dimensions = len(scan_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / max(1, num_scan_dimensions))\n",
    "        \n",
    "        join_column_cluster_counts = [CLUSTERS_PER_JOIN_COLUMN] * len(join_columns)\n",
    "        scan_column_cluster_counts = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        \n",
    "        # Merge join and scan columns\n",
    "        join_index = 0\n",
    "        scan_index = 0\n",
    "        cluster_counts = []\n",
    "        for clustering_column in clustering_columns:\n",
    "            if clustering_column in join_columns:\n",
    "                cluster_counts.append(join_column_cluster_counts[join_index])\n",
    "                join_index += 1\n",
    "            elif clustering_column in scan_columns:\n",
    "                cluster_counts.append(scan_column_cluster_counts[scan_index])\n",
    "                scan_index += 1\n",
    "        assert join_index == len(join_columns), f\"Processed the wrong number of join columns: {join_index} instead of {len(join_column_cluster_counts)}\"\n",
    "        assert scan_index == len(scan_columns), f\"Processed the wrong number of scan columns: {scan_index} instead of {len(scan_column_cluster_counts)}\"\n",
    "        assert len(cluster_counts) == len(clustering_columns), f\"Expected {len(clustering_columns)} cluster counts, but got {len(cluster_counts)}\"\n",
    "        \n",
    "        # incorporate that we cannot have more than MAX_HISTOGRAM_BINS clusters per column\n",
    "        max_cluster_counts = []\n",
    "        maximum_cluster_count_reached = []\n",
    "        for index, clustering_column in enumerate(clustering_columns):\n",
    "            max_clusters = min(MAX_HISTOGRAM_BINS, self.distinct_values[self.table_name][clustering_column])\n",
    "            max_cluster_counts.append(max_clusters)\n",
    "            if cluster_counts[index] > max_clusters:\n",
    "                #print(f\"Reducing cluster count for {clustering_column} from {cluster_counts[index]} to {max_clusters}\")\n",
    "                cluster_counts[index] = max_clusters\n",
    "            maximum_cluster_count_reached.append(cluster_counts[index] >= max_clusters)\n",
    "        \n",
    "        #total_cluster_count = np.prod(cluster_counts)\n",
    "        #growable_dimension_count = len(maximum_cluster_count_reached) - sum(maximum_cluster_count_reached)\n",
    "        #if total_cluster_count < target_cluster_count and growable_dimension_count > 1:\n",
    "        #    grow_factor = (target_cluster_count / total_cluster_count) ** (1/growable_dimension_count)\n",
    "        #    for index, clustering_column in enumerate(clustering_columns):\n",
    "        #        current = cluster_counts[index]\n",
    "        #        new = min(max_cluster_counts[index], math.floor(grow_factor * current))                \n",
    "        #        cluster_counts[index] = new\n",
    "                #if current < new:\n",
    "                #    print(f\"Increasing the cluster count of {clustering_column} from {current} to {new}\")\n",
    "                #else:\n",
    "                #    print(f\"Cannot increase the cluster count of {clustering_column}, because it already is at {current}\")\n",
    "                \n",
    "        \n",
    "        # testing\n",
    "        actual_cluster_count = reduce(operator.mul, cluster_counts, 1)\n",
    "        assert actual_cluster_count > 0, \"there was a split up factor of 0\"\n",
    "        assert actual_cluster_count <= 2 * target_cluster_count, f\"Wanted at most {target_cluster_count} clusters, but got {actual_cluster_count}\\nConfig: {clustering_columns}\\nCluster sizes: {cluster_counts}\"\n",
    "        estimated_chunksize = self.table_size / actual_cluster_count\n",
    "\n",
    "        #assert estimated_chunksize <= self.target_chunksize, f\"chunks should be smaller, not larger than target_chunksize. Estimated chunk size is {estimated_chunksize}\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return cluster_counts\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_dimension_cardinalities2(self, clustering_columns):\n",
    "        # ToDo what if we aim at less than number of chunks clusters, i.e. multiple chunks per cluster?\n",
    "        target_cluster_count = math.ceil(1.1 * self.table_size / self.target_chunksize)\n",
    "        # idea: fixed size for join columns, variable amount for scan columns\n",
    "        \n",
    "        join_columns = list(filter(lambda x: self.is_join_column(x), clustering_columns))\n",
    "        scan_columns = list(filter(lambda x: self.is_scan_column(x), clustering_columns))\n",
    "        intersecting_columns = set(join_columns).intersection(set(scan_columns))\n",
    "        assert len(intersecting_columns) == 0, f\"The following columns are used as both join and scan column: {intersecting_columns}\"\n",
    "        \n",
    "        if len(scan_columns) == 0:\n",
    "            CLUSTERS_PER_JOIN_COLUMN = math.ceil(math.pow(target_cluster_count, 1/len(join_columns)))\n",
    "        else: \n",
    "            CLUSTERS_PER_JOIN_COLUMN = 3;\n",
    "        # Assumption: uniform distribution (in the sense that every cluster actually exists)\n",
    "        num_join_clusters = math.pow(CLUSTERS_PER_JOIN_COLUMN, len(join_columns))\n",
    "        assert num_join_clusters <= 2 * target_cluster_count, f\"Would get {num_join_clusters} clusters for join columns, but aimed at at most {target_cluster_count} clusters\"\n",
    "    \n",
    "        # only applies to scan columns\n",
    "        desired_scan_clusters_count = math.ceil(target_cluster_count / num_join_clusters)\n",
    "        individual_distinct_values = [self.distinct_values[self.table_name][column] for column in scan_columns]\n",
    "        log_distinct_values = [math.ceil(0.5+np.log2(x)) for x in individual_distinct_values]\n",
    "        log_distinct_values_product = reduce(operator.mul, log_distinct_values, 1)\n",
    "        assert log_distinct_values_product > 0, \"cannot have a distinct value count of 0\"\n",
    "\n",
    "        global_modification_factor = desired_scan_clusters_count / log_distinct_values_product\n",
    "        num_scan_dimensions = len(scan_columns)\n",
    "        individual_modification_factor = np.power(global_modification_factor, 1.0 / max(1, num_scan_dimensions))\n",
    "        \n",
    "        join_column_cluster_counts = [CLUSTERS_PER_JOIN_COLUMN] * len(join_columns)\n",
    "        scan_column_cluster_counts = [math.ceil(x * individual_modification_factor) for x in log_distinct_values]\n",
    "        \n",
    "        \n",
    "        # Merge join and scan columns\n",
    "        join_index = 0\n",
    "        scan_index = 0\n",
    "        cluster_counts = []\n",
    "        for clustering_column in clustering_columns:\n",
    "            if clustering_column in join_columns:\n",
    "                cluster_counts.append(join_column_cluster_counts[join_index])\n",
    "                join_index += 1\n",
    "            elif clustering_column in scan_columns:\n",
    "                cluster_counts.append(scan_column_cluster_counts[scan_index])\n",
    "                scan_index += 1\n",
    "        assert join_index == len(join_columns), f\"Processed the wrong number of join columns: {join_index} instead of {len(join_column_cluster_counts)}\"\n",
    "        assert scan_index == len(scan_columns), f\"Processed the wrong number of scan columns: {scan_index} instead of {len(scan_column_cluster_counts)}\"\n",
    "        assert len(cluster_counts) == len(clustering_columns), f\"Expected {len(clustering_columns)} cluster counts, but got {len(cluster_counts)}\"\n",
    "        \n",
    "        # testing\n",
    "        actual_cluster_count = reduce(operator.mul, cluster_counts, 1)\n",
    "        assert actual_cluster_count > 0, \"there was a split up factor of 0\"\n",
    "        assert actual_cluster_count <= 2 * target_cluster_count, f\"Wanted at most {target_cluster_count} clusters, but got {actual_cluster_count}\\nConfig: {clustering_columns}\\nCluster sizes: {cluster_counts}\"\n",
    "        estimated_chunksize = self.table_size / actual_cluster_count\n",
    "\n",
    "        assert estimated_chunksize <= self.target_chunksize, f\"chunks should be smaller, not larger than target_chunksize. Estimated chunk size is {estimated_chunksize}\"\n",
    "        allowed_percentage = 0.55\n",
    "        if estimated_chunksize < allowed_percentage * self.target_chunksize:\n",
    "            print(f\"Warning: chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\")\n",
    "        #assert estimated_chunksize >= allowed_percentage * self.target_chunksize, f\"chunks should not be too much smaller than target_chunksize: {estimated_chunksize} < {allowed_percentage} * {self.target_chunksize}\"\n",
    "        \n",
    "        return cluster_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhatIfModel(DisjointClustersModel):\n",
    "    \n",
    "    def __init__(self, max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation, model_dir='cost_model_output/models/', model_type='boost'):\n",
    "        super().__init__(max_dimensions, query_frequencies, table_name, table_scans, table_sizes, distinct_values, target_chunksize, correlations, joins, sorted_columns_during_creation)\n",
    "        self.models = util.load_models(model_dir)\n",
    "        self.model_formats = util.load_model_input_formats(model_dir)\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    \n",
    "    def estimate_table_scan_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):        \n",
    "        runtime = 0\n",
    "        self.table_scans = self.table_scans.copy()\n",
    "        self.table_scans['OPERATOR_IMPLEMENTATION'] = self.table_scans.apply(lambda x: x['DESCRIPTION'].split(\"Impl: \")[1].split()[0], axis=1)\n",
    "        self.table_scans['RUNTIME_ESTIMATE'] = -1\n",
    "        \n",
    "        scans_by_implementation = self.table_scans.groupby(['OPERATOR_IMPLEMENTATION'])\n",
    "        for operator_implementation, df in scans_by_implementation:\n",
    "            print(f\"## Estimating {operator_implementation} scans\")\n",
    "            model_name = util.get_table_scan_model_name(self.model_type, operator_implementation)\n",
    "            model = self.models[model_name]\n",
    "            \n",
    "            df = df.copy()                        \n",
    "            df = df.rename(columns={\n",
    "                'selectivity': 'SELECTIVITY_LEFT',\n",
    "                'INPUT_ROW_COUNT': 'INPUT_ROWS',\n",
    "                'OUTPUT_ROW_COUNT': 'OUTPUT_ROWS',\n",
    "                'PREDICATE_CONDITION': 'PREDICATE',\n",
    "                'INPUT_CHUNK_COUNT': 'INPUT_CHUNKS',\n",
    "            })\n",
    "            \n",
    "            df['INPUT_COLUMN_SORTED'] = df.apply(lambda x: \"Ascending\" if x['COLUMN_NAME'] == sorting_column else \"No\", axis=1)\n",
    "            \n",
    "            df = df.drop(columns=['RUNTIME_ESTIMATE', 'COLUMN_NAME', 'DESCRIPTION', 'GET_TABLE_HASH', 'LEFT_INPUT_OPERATOR_HASH', 'OPERATOR_HASH', 'OPERATOR_TYPE', 'OUTPUT_CHUNK_COUNT', 'QUERY_HASH', 'RIGHT_INPUT_OPERATOR_HASH', 'RUNTIME_NS', 'SCANS_SKIPPED', 'SCANS_SORTED', 'TABLE_NAME', 'benefits_from_sorting', 'part_of_or_chain', 'time_per_input_row', 'time_per_output_row', 'time_per_row', 'useful_for_pruning', 'OPERATOR_IMPLEMENTATION'])\n",
    "            df = util.preprocess_data(df)\n",
    "            df = util.append_to_input_format(df, self.model_formats[model_name])\n",
    "            #df = df.drop(columns=['RUNTIME_NS'])\n",
    "            \n",
    "            predictions = model.predict(df)\n",
    "            self.scan_estimates.loc[df.index, 'RUNTIME_ESTIMATE'] = np.array(predictions, dtype=np.int64)\n",
    "            \n",
    "            runtime += predictions.sum()\n",
    "        print()\n",
    "            \n",
    "        return runtime\n",
    "        \n",
    "    def estimate_join_runtime(self, clustering_columns, sorting_column, dimension_cardinalities):\n",
    "        runtime = 0\n",
    "        \n",
    "        joins = self.joins.copy()\n",
    "        joins['OPERATOR_IMPLEMENTATION'] = joins.apply(lambda x: x['DESCRIPTION'].split()[0], axis=1)\n",
    "        \n",
    "        joins['BUILD_COLUMN_TYPE'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_COLUMN_TYPE\"], axis=1)\n",
    "        joins['BUILD_INPUT_CHUNKS'] = joins.apply(lambda x: x[x['BUILD_SIDE'] + \"_TABLE_CHUNK_COUNT\"], axis=1)\n",
    "        joins['PROBE_COLUMN_TYPE'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_COLUMN_TYPE\"], axis=1)\n",
    "        joins['PROBE_INPUT_CHUNKS'] = joins.apply(lambda x: x[x['PROBE_SIDE'] + \"_TABLE_CHUNK_COUNT\"], axis=1)\n",
    "        \n",
    "        joins_by_implementation = joins.groupby(['OPERATOR_IMPLEMENTATION', 'BUILD_COLUMN_TYPE', 'PROBE_COLUMN_TYPE'])\n",
    "        for (implementation, build_type, probe_type), df in joins_by_implementation:\n",
    "            print(f\"## Estimating {build_type} {probe_type} joins\")\n",
    "            model_name = util.get_join_model_name(self.model_type, implementation, build_type, probe_type)\n",
    "            model = self.models[model_name]\n",
    "            \n",
    "            df = df.copy()\n",
    "            df = df.drop(columns=['OPERATOR_IMPLEMENTATION', 'PREDICATE_COUNT', 'PRIMARY_PREDICATE'])            \n",
    "            df = util.preprocess_data(df)\n",
    "            df = df.rename(columns={\n",
    "                'PROBE_TABLE_ROW_COUNT': 'PROBE_INPUT_ROWS',\n",
    "                'BUILD_TABLE_ROW_COUNT': 'BUILD_INPUT_ROWS',\n",
    "                'OUTPUT_ROW_COUNT': 'OUTPUT_ROWS',\n",
    "                'BUILD_SORTED_0': 'BUILD_INPUT_COLUMN_SORTED_No',\n",
    "                'BUILD_SORTED_1': 'BUILD_INPUT_COLUMN_SORTED_Ascending',\n",
    "                'PROBE_SORTED_0': 'PROBE_INPUT_COLUMN_SORTED_No',\n",
    "                'PROBE_SORTED_1': 'PROBE_INPUT_COLUMN_SORTED_Ascending'\n",
    "            })\n",
    "\n",
    "            df = df.drop(columns=['BUILDING_NS', 'BUILD_COLUMN', 'BUILD_SIDE', 'BUILD_SIDE_MATERIALIZING_NS', 'BUILD_TABLE', 'CLUSTERING_NS', 'DESCRIPTION', 'IS_FLIPPED', 'LEFT_COLUMN_NAME', 'LEFT_INPUT_OPERATOR_HASH', 'LEFT_TABLE_CHUNK_COUNT', 'LEFT_TABLE_NAME', 'LEFT_TABLE_ROW_COUNT', 'OPERATOR_HASH', 'OPERATOR_TYPE', 'OUTPUT_CHUNK_COUNT', 'OUTPUT_WRITING_NS', 'PROBE_COLUMN', 'PROBE_SIDE', 'PROBE_SIDE_MATERIALIZING_NS', 'PROBE_TABLE', 'PROBING_NS', 'QUERY_HASH', 'RADIX_BITS', 'RIGHT_COLUMN_NAME', 'RIGHT_COLUMN_TYPE_REFERENCE', 'RIGHT_INPUT_OPERATOR_HASH', 'RIGHT_TABLE_CHUNK_COUNT', 'RIGHT_TABLE_NAME', 'RIGHT_TABLE_ROW_COUNT'])\n",
    "            optional_drop_columns = ['LEFT_COLUMN_TYPE_DATA', 'LEFT_COLUMN_TYPE_REFERENCE', 'RUNTIME_NS', 'BUILD_INPUT_COLUMN_SORTED_Ascending']\n",
    "            df = df.drop(columns=optional_drop_columns, errors='ignore')\n",
    "            df = util.append_to_input_format(df, self.model_formats[model_name])\n",
    "            \n",
    "            predictions = model.predict(df)\n",
    "            self.join_estimates.loc[df.index, 'RUNTIME_ESTIMATE'] = np.array(predictions, dtype=np.int64)\n",
    "\n",
    "            runtime += predictions.sum()\n",
    "        print()\n",
    "                        \n",
    "        return runtime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = create_model(\"lineitem\", 2)\n",
    "#m.suggest_clustering(30)\n",
    "m.estimate_total_runtime(['l_orderkey'], 'l_orderkey', [100])\n",
    "#m.table_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#model.suggest_clustering(20)\n",
    "#model.estimate_distinct_values_per_chunk_at_statistics_time(\"l_orderkey\", \"lineitem\")\n",
    "#print(model.estimate_distinct_values_per_chunk(\"l_orderkey\", [\"l_orderkey\", \"l_partkey\"], \"l_orderkey\", [20,5]))\n",
    "\n",
    "#model.estimate_chunk_count(\"9762c3a887e47469\", 77313, [\"l_orderkey\"], [92])\n",
    "#row = model.joins.iloc[140]\n",
    "#model.get_chunk_count_factor(row, \"BUILD\", [\"l_orderkey\", \"l_shipdate\"], [5, 20])\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "  \n",
    "def create_benchmark_configs(table_name=None, max_dimension=2, firstk=20):\n",
    "    counter = 0\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    clusterings = {}#{\"default\" : default_benchmark_config()}\n",
    "    query_frequencies = get_query_frequencies()\n",
    "    \n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    if table_name is not None:\n",
    "        table_names = [table_name]\n",
    "    else:\n",
    "        table_names = get_table_names(scans, joins)\n",
    "    \n",
    "    print(table_names)\n",
    "    for table_name in table_names:        \n",
    "        start_time_table = datetime.now()\n",
    "        single_table_scans = extract_single_table(scans, table_name)\n",
    "        table_size = table_sizes[table_name]\n",
    "        if table_size <= 3 * CHUNK_SIZE:\n",
    "            print(f\"Not computing clustering for {table_name}, as it has only {table_size} rows\")\n",
    "            continue        \n",
    "        \n",
    "        model = DisjointClustersModel(max_dimension, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "        table_clusterings = model.suggest_clustering(firstk)\n",
    "        print(json.dumps(table_clusterings, indent=2))\n",
    "        for table_clustering in table_clusterings:\n",
    "            counter += 1\n",
    "            #config = default_benchmark_config()\n",
    "            config = {}\n",
    "            config[table_name] = format_table_clustering(table_clustering)\n",
    "            config_name = '{:02d}-'.format(counter) +  get_config_name(config)\n",
    "            clusterings[config_name] = config\n",
    "        end_time_table = datetime.now()\n",
    "        print(f\"Done computing clustering for {table_name} ({end_time_table - start_time_table})\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Computed all clusterings in {end_time - start_time}\")\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "#create_benchmark_configs(table_name=\"lineitem\", max_dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(table_name, max_dimensions=2):    \n",
    "    query_frequencies = get_query_frequencies()\n",
    "    distinct_values = get_distinct_values_count()\n",
    "    joins = load_join_statistics()    \n",
    "    sorted_columns_during_creation = get_sorted_columns_during_creation()\n",
    "    correlations = get_correlations()\n",
    "    table_names = get_table_names(scans, joins)\n",
    "    start_time_table = datetime.now()\n",
    "    single_table_scans = extract_single_table(scans, table_name)\n",
    "\n",
    "    model = WhatIfModel(max_dimensions, query_frequencies, table_name, single_table_scans, table_sizes, distinct_values, CHUNK_SIZE, correlations.get(table_name, {}), joins, sorted_columns_during_creation)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(values, bins):\n",
    "    def elements_between(low, high, upper_exclusive=True):\n",
    "        if upper_exclusive:        \n",
    "            filtered = filter(lambda x: low <= x and x < high, values)\n",
    "        else:\n",
    "            filtered = filter(lambda x: low <= x and x <= high, values)\n",
    "            \n",
    "        return len(list(filtered))\n",
    "    \n",
    "    return [elements_between(low, high ) for low, high in zip(bins, bins[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_join_step(table_name, step, sided=True):\n",
    "    side = None\n",
    "    if step == \"BUILD_SIDE_MATERIALIZING\" or step == \"BUILDING\":\n",
    "        side = \"BUILD\"\n",
    "    elif step == \"PROBE_SIDE_MATERIALIZING\" or step == \"PROBING\":\n",
    "        side = \"PROBE\"\n",
    "    \n",
    "    \n",
    "    if step == \"ALL\":\n",
    "        measured_column = \"RUNTIME_NS\"\n",
    "        estimate_column = \"RUNTIME_ESTIMATE\"\n",
    "    else:\n",
    "        measured_column = step + \"_NS\"\n",
    "        estimate_column = \"ESTIMATE_\" + step\n",
    "    \n",
    "    \n",
    "    m = create_model(table_name, 2)\n",
    "    \n",
    "    m.estimate_total_runtime([CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "    #m.estimate_total_runtime([\"l_shipdate\", \"l_orderkey\"], \"l_orderkey\", [20, 50])\n",
    "    #m.estimate_total_runtime([\"ss_sold_time_sk\"], \"ss_ticket_number\", [100])\n",
    "    \n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_BUILD_SIDE_MATERIALIZING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_PROBE_SIDE_MATERIALIZING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_CLUSTERING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_BUILDING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_PROBING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    #assert len(m.join_estimates[m.join_estimates['ESTIMATE_OUTPUT_WRITING'] < 0]) == 0, \"not all runtimes computed\"\n",
    "    print(f\"there are {len(m.join_estimates[m.join_estimates['RUNTIME_ESTIMATE'] < 0])} unestimated joins\")\n",
    "    print(m.join_estimates[m.join_estimates['RUNTIME_ESTIMATE'] < 0])\n",
    "    assert len(m.join_estimates[m.join_estimates['RUNTIME_ESTIMATE'] == -1]) == 0, \"not all runtimes computed\"\n",
    "    \n",
    "    CLUSTERED_STATISTICS_PATH = f\"/home/aloeser/Dokumente/repos/example_plugin/TPC-H__SF_1.000000__RUNS_10__TIME_5000__ENCODING_DictionaryFSBA\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"/home/aloeser/Dokumente/repos/example_plugin/stats/final/tpch/sf10_r10/{CLUSTERING_COLUMN}-100\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"/home/aloeser/Dokumente/repos/example_plugin/stats/final/tpch/sf10_r10/l_shipdate-20_l_orderkey-50\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"/home/aloeser/Dokumente/repos/example_plugin/stats/final/tpcds/sf10-2d/04-ss_sold_time_sk-100_ss_ticket_number-1/\"\n",
    "    clustered_joins = load_join_statistics(path=CLUSTERED_STATISTICS_PATH)\n",
    "    clustered_joins = clustered_joins.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    print(f\"there are {len(m.joins)} joins in the model\")\n",
    "    print(f\"there are {len(clustered_joins)} clustered joins\")\n",
    "    \n",
    "    frequencies = get_query_frequencies()\n",
    "    current_sum = m.joins.apply(lambda x: x[measured_column] * frequencies[x['QUERY_HASH']], axis=1).sum()\n",
    "    estimate_sum = int(m.join_estimates.apply(lambda x: x[estimate_column] * frequencies[x['QUERY_HASH']], axis=1).sum())\n",
    "    new_sum = clustered_joins.apply(lambda x: x[measured_column] * frequencies[x['QUERY_HASH']], axis=1).sum()\n",
    "    \n",
    "    print(f\"current : {current_sum}\")\n",
    "    print(f\"estimate: {estimate_sum}\")\n",
    "    print(f\"new     : {new_sum}\")\n",
    "    \n",
    "    if side is not None and sided:\n",
    "        clustered_joins = clustered_joins[clustered_joins[f\"{side}_TABLE\"] == table_name]\n",
    "        print(f\"{len(clustered_joins)} joins with {table_name} as the {side} side\")\n",
    "    \n",
    "    m.join_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    estimates = m.join_estimates.loc[clustered_joins.index]\n",
    "    assert len(estimates) == len(clustered_joins), \"lengths do not match\"\n",
    "    #assert len(estimates) == len(estimates[estimates[f\"{side}_TABLE\"] == table_name]), \"did not get all rows\"\n",
    "\n",
    "    if side is not None and sided:\n",
    "        current_sum = m.joins.loc[clustered_joins.index].apply(lambda x: x[measured_column] * frequencies[x['QUERY_HASH']], axis=1).sum()\n",
    "        estimate_sum = int(estimates.apply(lambda x: x[estimate_column] * frequencies[x['QUERY_HASH']], axis=1).sum())\n",
    "        new_sum = clustered_joins.apply(lambda x: x[measured_column] * frequencies[x['QUERY_HASH']], axis=1).sum()\n",
    "        \n",
    "        print(f\"current : {current_sum}\")\n",
    "        print(f\"estimate: {estimate_sum}\")\n",
    "        print(f\"new     : {new_sum}\")\n",
    "          \n",
    "    result = pd.DataFrame()\n",
    "    if side is not None and sided:\n",
    "        result['COLUMN_NAME'] = np.array(clustered_joins[f\"{side}_COLUMN\"])\n",
    "    result['DESCRIPTION1'] = np.array(estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_joins['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_joins['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(estimates[measured_column])\n",
    "    #result['RUNTIME_ESTIMATE'] = np.array(estimates[f\"ESTIMATE_{step}\"], dtype=np.int64)\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(estimates[estimate_column], dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_joins[measured_column])\n",
    "    \n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    if step == \"CLUSTERING\":\n",
    "        result['RELATIVE_ERROR'].fillna(1, inplace=True)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_shipdate\"\n",
    "SIDED = False\n",
    "results = eval_join_step(\"lineitem\", \"ALL\", SIDED)\n",
    "#results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] == 'l_orderkey']\n",
    "\n",
    "print(results['RELATIVE_ERROR'].min())\n",
    "print(results['RELATIVE_ERROR'].max())\n",
    "#results = results[results['RUNTIME_BASE'] > 5e8]\n",
    "#print(f\"there are {len(results)} joins\")\n",
    "#results[results['QUERY_HASH1'] == \"306bc6df7241ccf9\"]\n",
    "#[results['RUNTIME_BASE'] != results['RUNTIME_ESTIMATE']]\n",
    "results[['QUERY_HASH1', 'DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'TOTAL_ERROR', 'RELATIVE_ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "frequencies = get_query_frequencies()\n",
    "\n",
    "all_factors = []\n",
    "for _, row in results.iterrows():\n",
    "    frequency = frequencies[row['QUERY_HASH1']]\n",
    "    for _ in range(frequency):\n",
    "        all_factors.append(row['RELATIVE_ERROR'])\n",
    "#factors = results['RELATIVE_ERROR']\n",
    "\n",
    "#factors = list(map(lambda x: x if x >= 0 else -1/x, factors))\n",
    "\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.3, 1.5, 1.8, 2.1, 2.5, 5, 10, 30]\n",
    "\n",
    "values = histogram(all_factors, bins)\n",
    "print(values)\n",
    "thirty_percent_range = values[7] + values[8] + values[9] + values[10]\n",
    "print(f\"within 30%: {thirty_percent_range}/{sum(values)} ({100*thirty_percent_range/sum(values)}%)\")\n",
    "x = np.arange(len(values))\n",
    "labels = list([f\"[{low},{high})\" for low, high in zip(bins, bins[1:])])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel(\"Relative estimation error\")\n",
    "ax.set_ylabel(\"Number of joins\")\n",
    "ax.bar(x, values)\n",
    "ax.get_yaxis().set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(rotation=90)\n",
    "#plt.title(\"Histogram of relative estimation errors\")\n",
    "\n",
    "\n",
    "extension = '' if SIDED else '_all'\n",
    "\n",
    "#plt.savefig(f'/home/aloeser/Downloads/MA-Figures/pngs/operatoreval/join_materialize_probe_shipdate{extension}.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_bounds = [[1, 1.5], [1.5, 3], [3, 100]]\n",
    "percentiles = []\n",
    "\n",
    "for bound in percentile_bounds:\n",
    "    mi = bound[0]\n",
    "    ma = bound[1]\n",
    "    high = results[results['RELATIVE_ERROR'] < ma]\n",
    "    high = high[high['RELATIVE_ERROR'] >= mi]\n",
    "    low = results[results['RELATIVE_ERROR'] <= 1/mi]\n",
    "    low = low[low['RELATIVE_ERROR'] > 1/ma]    \n",
    "    percentiles.append(len(low) + len(high))\n",
    "    \n",
    "np.array(percentiles) * 100 / len(results)\n",
    "#33 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "ESTIMATE_COLUMN = \"RUNTIME_ESTIMATE\"\n",
    "MEASURED_COLUMN = \"RUNTIME_CLUSTERED\"\n",
    "\n",
    "frequencies = get_query_frequencies()\n",
    "\n",
    "estimates_ms = results.apply(lambda x: x[ESTIMATE_COLUMN] * frequencies[x['QUERY_HASH1']] / 1e6, axis=1)\n",
    "measured_ms = results.apply(lambda x: x[MEASURED_COLUMN] * frequencies[x['QUERY_HASH1']] / 1e6, axis=1)\n",
    "\n",
    "\n",
    "mse_nanoseconds = sklearn.metrics.mean_squared_error(measured_ms, estimates_ms)\n",
    "#sum_relative_error = results[ESTIMATE_COLUMN].sum / results(MEASURED_COLUMN).sum()\n",
    "\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200 * np.mean(diff)\n",
    "\n",
    "print(f\"there are {len(results)} operators\")\n",
    "print(f\"total estimate: {estimates_ms.sum()}\")\n",
    "print(f\"total measured: {measured_ms.sum()}\")\n",
    "print(f\"MSE: {mse_nanoseconds}\")\n",
    "print(f\"SMAPE: {smape(measured_ms, estimates_ms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scans(table_name):\n",
    "    m = create_model(table_name, 2)\n",
    "\n",
    "    m.estimate_total_runtime([CLUSTERING_COLUMN], CLUSTERING_COLUMN, [100])\n",
    "    #m.estimate_total_runtime(['l_shipdate', 'l_orderkey'], 'l_orderkey', [20, 50])\n",
    "    assert len(m.scan_estimates[m.scan_estimates['RUNTIME_ESTIMATE'] < 0]) == 0, \"not all runtimes computed\"\n",
    "\n",
    "    CLUSTERED_STATISTICS_PATH = \"~/Dokumente/repos/example_plugin/TPC-H__SF_1.000000__RUNS_10__TIME_5000__ENCODING_DictionaryFSBA\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/stats/final/tpch/sf10_r10/{CLUSTERING_COLUMN}-100\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"~/Dokumente/repos/example_plugin/stats/final/tpch/sf10_r10/l_shipdate-20_l_orderkey-50\"\n",
    "    #CLUSTERED_STATISTICS_PATH = f\"/home/aloeser/Dokumente/repos/example_plugin/stats/final/tpcds/sf10-2d/04-ss_sold_time_sk-100_ss_ticket_number-1\"\n",
    "    \n",
    "    path = f\"{CLUSTERED_STATISTICS_PATH}/table_scans.csv\"\n",
    "    clustered_scans = pd.read_csv(path, sep='|')\n",
    "    clustered_scans = clustered_scans[clustered_scans['TABLE_NAME'] == table_name]\n",
    "    clustered_scans = clustered_scans.sort_values(['QUERY_HASH', 'DESCRIPTION'])\n",
    "    \n",
    "    print(f\"there are {len(m.scan_estimates)} scan estimates\")\n",
    "    print(f\"there are {len(clustered_scans)} clustered scans\")\n",
    "    \n",
    "    m.scan_estimates.sort_values(['QUERY_HASH', 'DESCRIPTION'], inplace=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['QUERY_HASH'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['rti'] = np.array(m.scan_estimates['time_per_input_row'])\n",
    "    result['COLUMN_NAME'] = np.array(clustered_scans['COLUMN_NAME'])\n",
    "    result['DESCRIPTION1'] = np.array(m.scan_estimates['DESCRIPTION'])\n",
    "    result['DESCRIPTION2'] = np.array(clustered_scans['DESCRIPTION'])\n",
    "    result['QUERY_HASH1'] = np.array(m.scan_estimates['QUERY_HASH'])\n",
    "    result['QUERY_HASH2'] = np.array(clustered_scans['QUERY_HASH'])\n",
    "    result['RUNTIME_BASE'] = np.array(m.scan_estimates['RUNTIME_NS'])\n",
    "    result['RUNTIME_ESTIMATE'] = np.array(m.scan_estimates.apply(lambda row: row['RUNTIME_ESTIMATE'] / m.query_frequency(row['QUERY_HASH']), axis=1), dtype=np.int64)\n",
    "    result['RUNTIME_CLUSTERED'] = np.array(clustered_scans['RUNTIME_NS'])\n",
    "    \n",
    "    # make sure we match all operators\n",
    "    matches = result.apply(lambda row: row['DESCRIPTION1'] == row['DESCRIPTION2'] and row['QUERY_HASH1'] == row['QUERY_HASH2'], axis=1)\n",
    "    assert matches.all(), \"not all rows match\"\n",
    "    \n",
    "    result['TOTAL_ERROR'] = result['RUNTIME_CLUSTERED'] - result['RUNTIME_ESTIMATE']\n",
    "    result['RELATIVE_ERROR'] = result['RUNTIME_CLUSTERED'] / result['RUNTIME_ESTIMATE']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "CLUSTERING_COLUMN = \"l_orderkey\"\n",
    "results = eval_scans(\"lineitem\")\n",
    "print(f\" there are {len(results)} scans\")\n",
    "#results = results[CLUSTERING_COLUMN == results['COLUMN_NAME']]\n",
    "#results = results[\"l_quantity\" == results['COLUMN_NAME']]\n",
    "#results = results[results['COLUMN_NAME'] == 'l_receiptdate']\n",
    "#results = results[results['RELATIVE_ERROR'] != 1.0]\n",
    "results[['QUERY_HASH', 'DESCRIPTION1', 'RUNTIME_BASE', 'RUNTIME_ESTIMATE', 'RUNTIME_CLUSTERED', 'rti', 'TOTAL_ERROR', 'RELATIVE_ERROR']].sort_values(['RUNTIME_CLUSTERED'], ascending=False)\n",
    "#results[['QUERY_HASH', 'RUNTIME_CLUSTERED']].sort_values(['RUNTIME_CLUSTERED'], ascending=False)\n",
    "#results.to_csv('detailed_estimates_shipdate_orderkey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "frequencies = get_query_frequencies()\n",
    "\n",
    "all_factors = []\n",
    "for _, row in results.iterrows():\n",
    "    frequency = frequencies[row['QUERY_HASH1']]\n",
    "    for _ in range(frequency):\n",
    "        all_factors.append(row['RELATIVE_ERROR'])\n",
    "#factors = results['RELATIVE_ERROR']\n",
    "\n",
    "#factors = list(map(lambda x: x if x >= 0 else -1/x, factors))\n",
    "\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.3, 1.5, 1.8, 2.1, 2.5, 5, 10, 50, 500]\n",
    "\n",
    "values = histogram(all_factors, bins)\n",
    "print(values)\n",
    "thirty_percent_range = values[7] + values[8] + values[9] + values[10]\n",
    "print(f\"within 30%: {thirty_percent_range}/{sum(values)} ({100*thirty_percent_range/sum(values)}%)\")\n",
    "\n",
    "x = np.arange(len(values))\n",
    "labels = list([f\"[{low},{high})\" for low, high in zip(bins, bins[1:])])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel(\"Relative estimation error\")\n",
    "ax.set_ylabel(\"Number of scans\")\n",
    "ax.bar(x, values)\n",
    "ax.get_yaxis().set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(rotation=90)\n",
    "#plt.title(\"Histogram of relative estimation errors\")\n",
    "\n",
    "#plt.savefig(f'/home/aloeser/Downloads/operatoreval/scan_partkey.pdf', bbox_inches='tight')\n",
    "#plt.savefig(f'/home/aloeser/Downloads/MA-Figures/pngs/operatoreval/scan_shipdate.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_bounds = [[1, 1.5], [1.5, 3], [3, 100]]\n",
    "percentiles = []\n",
    "\n",
    "for bound in percentile_bounds:\n",
    "    mi = bound[0]\n",
    "    ma = bound[1]\n",
    "    high = results[results['RELATIVE_ERROR'] < ma]\n",
    "    high = high[high['RELATIVE_ERROR'] >= mi]\n",
    "    low = results[results['RELATIVE_ERROR'] <= 1/mi]\n",
    "    low = low[low['RELATIVE_ERROR'] > 1/ma]    \n",
    "    percentiles.append(len(low) + len(high))\n",
    "    \n",
    "np.array(percentiles) * 100 / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_correct_statistics_loaded()\n",
    "\n",
    "def extract_single_table(table_scans, table_name):\n",
    "    return table_scans[table_scans['TABLE_NAME'] == table_name]\n",
    "\n",
    "def default_benchmark_config():    \n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        config = {\n",
    "            'lineitem': [['l_shipdate', 2]],\n",
    "            'orders': [['o_orderdate', 2]]\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        config = dict()\n",
    "    else:        \n",
    "        raise Exception(\"unknown benchmark, please provide a default config\")\n",
    "    return config\n",
    "\n",
    "def get_correlations():\n",
    "    if BENCHMARK == \"TPCH\":\n",
    "        correlations = {\n",
    "            'lineitem': {\n",
    "                'l_shipdate': ['l_receiptdate', 'l_commitdate'],\n",
    "                'l_receiptdate': ['l_shipdate', 'l_commitdate'],\n",
    "            }\n",
    "        }\n",
    "    elif BENCHMARK == \"TPCDS\":\n",
    "        correlations = dict()\n",
    "    else:\n",
    "        raise Exception(\"unknown benchmark, please provide correlation information\")\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def format_table_clustering(clustering_config):\n",
    "    # input format: List of [ [(column, split)+ ], sorting_column, runtime ]\n",
    "    # output format: List of [ (column, split)+ ] - sorting column integrated if necessary\n",
    "    \n",
    "    assert len(clustering_config) == 3, \"config should have exactly three entries: clustering columns, sort column, runtime\"\n",
    "    clustering_columns = clustering_config[0]\n",
    "    assert len(clustering_columns) <= 3, \"atm the model is at most 3-dimensional\"\n",
    "    #print(f\"clustering columns are {clustering_columns}\")\n",
    "    last_clustering_column = clustering_columns[-1]\n",
    "    last_clustering_column_name = last_clustering_column[0]\n",
    "    #print(f\"last column is {last_clustering_column_name}\")\n",
    "    sorting_column = clustering_config[1]\n",
    "    #print(f\"sort column is {sorting_column}\")\n",
    "    \n",
    "    result = clustering_columns\n",
    "    if last_clustering_column_name != sorting_column:\n",
    "        result = clustering_columns + [(sorting_column, 1)]\n",
    "        \n",
    "    #print(f\"in: {clustering_config}\")\n",
    "    #print(f\"out: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_config_name(clustering_config):\n",
    "    # Input: config-dict\n",
    "    \n",
    "    # List of lists. Each secondary list contains clustering information for a table\n",
    "    table_configs = [clustering_config[table] for table in clustering_config]\n",
    "    config_entries = [[f\"{config_entry[0]}-{config_entry[1]}\" for config_entry in config] for config in table_configs]\n",
    "    table_entries = [\"_\".join(config) for config in config_entries]\n",
    "    return \"_\".join(table_entries)\n",
    "\n",
    "\n",
    "#model = create_model(\"lineitem\", 2)\n",
    "#model.table_scans['original_unprunable_part'] = np.array(model.table_scans['original_unprunable_part'], dtype=np.int32)\n",
    "#model.table_scans[model.table_scans['COLUMN_NAME'] == 'o_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdated code fragments (older model versions) are kept below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"This assertion failure only serves to stop execution here when clicking Cells->Run all. You can safely ignore it.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
